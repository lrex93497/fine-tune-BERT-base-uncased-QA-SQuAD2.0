{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we do prediction, then we do classifier by other classifier for unanswerable question\n",
    "#as we find by finding [CLS] output for unanswerable question has low accuracy to idenify it.\n",
    "\n",
    "#for classifier we had refered to https://github.com/ThaddeusSegura/BERT_on_SQuAD/blob/master/SE_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for showing progress bar\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertForSequenceClassification\n",
    "from transformers import BertTokenizerFast, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#Using torch by GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Use cuda device:\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"use cpu\")\n",
    "\n",
    "#set dataset location\n",
    "SQuAD = pd.read_csv('dev-v2.0-combined-use.csv')\n",
    "#setoutput file name\n",
    "output_name = \"output-dev.json\"\n",
    "#checkpoint path\n",
    "model_qa = BertForQuestionAnswering.from_pretrained('./bert_qa_pt_3/', local_files_only=True)\n",
    "#tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "#set batch_size here, This batch size just deside how many dataset to gpu at once\n",
    "#but gpu still good though them one by one, this should low vram usage\n",
    "batch_size_qa = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(dataset):\n",
    "    #convet values from dataframe to list for tokenizer\n",
    "    questions = dataset['question'].values.tolist()\n",
    "    contexts = dataset['context'].values.tolist()\n",
    "    id = dataset['id'].values.tolist()\n",
    "    #answers = dataset['text']\n",
    "    \n",
    "    return {\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'id': id,\n",
    "        #'answers': answers\n",
    "    }\n",
    "\n",
    "dataset = prep_data(SQuAD)\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "question_id_list = []\n",
    "type_ids_list = []\n",
    "\n",
    "for dataset_question, dataset_context in zip(dataset['question'], dataset['context']):\n",
    "    #do two times as by one line cannot get all values\n",
    "    \n",
    "    tokenized__for_index = tokenizer.encode(dataset_context,dataset_question,truncation=True,padding='max_length')                                 \n",
    "    tokenized = tokenizer.encode_plus(dataset_context,\n",
    "                            dataset_question,\n",
    "                            add_special_tokens=True,    # Add `[CLS]` and `[SEP]`\n",
    "                            truncation=True,\n",
    "                            return_attention_mask=True,  # Construct attn. masks.\n",
    "                            padding='max_length',\n",
    "                            return_tensors='pt')\n",
    "    \n",
    "    #find location of sep\n",
    "    sep_index = tokenized__for_index.index(tokenizer.sep_token_id)\n",
    "\n",
    "    #segment a which contains sep itself\n",
    "    segment_a_no = sep_index + 1\n",
    "\n",
    "    #segment b the rest of the ids\n",
    "    segment_b_no = len(tokenized__for_index) - segment_a_no\n",
    "\n",
    "    #make a list, 0 for segment a, 1 for segment b.\n",
    "    segment_ids = [0] * segment_a_no + [1] * segment_b_no\n",
    "    #padding is handled by attention mask\n",
    "    #check if segment_ids is normal\n",
    "    assert len(tokenized__for_index) == len(segment_ids)\n",
    "\n",
    "    type_ids = segment_ids\n",
    "    type_ids_list.append(type_ids)\n",
    "    input_ids_list.append(tokenized['input_ids'])\n",
    "    attention_mask_list.append(tokenized['attention_mask'])\n",
    "\n",
    "for id in dataset['id']:\n",
    "    question_id_list.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(map(len,type_ids_list)))==1:\n",
    "    print(\"All are the same length\")\n",
    "else:\n",
    "    print(\"They are not the same length!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question id list making\n",
    "for id in dataset['id']:\n",
    "    question_id_list.append(id)\n",
    "    \n",
    "#to tensor\n",
    "input_ids_list = torch.stack(input_ids_list)\n",
    "attention_mask_list = torch.stack(attention_mask_list)\n",
    "type_ids_list = torch.Tensor(type_ids_list).to(torch.int64)\n",
    "\n",
    "#pack dataset into data loader\n",
    "data_3_elements = TensorDataset(input_ids_list, attention_mask_list, type_ids_list)\n",
    "dataloader = DataLoader(data_3_elements, sampler=None, batch_size=batch_size_qa, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa.to(device)        #put qa model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run and get result from model \n",
    "answers_list = []\n",
    "for batch in tqdm(dataloader):\n",
    "    #batch = torch.stack(batch)\n",
    "    #batch = batch.to(device)\n",
    "    \n",
    "    batch_input, batch_mask, batch_type = tuple(v.to(device) for v in batch)\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_mask = batch_mask.to(device)\n",
    "    batch_type = batch_type.to(device)\n",
    "    for input, mask, type in zip(batch_input, batch_mask, batch_type):\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input[0])  #[1] is device=\"device\"\n",
    "        result = model_qa(input_ids = input, attention_mask=mask, token_type_ids=type)       #use ids and mask and type to get result\n",
    "        answer_start = torch.argmax(result.start_logits)    #get start position by argmax\n",
    "        answer_end = torch.argmax(result.end_logits)        #get end position by argmax\n",
    "\n",
    "        # join the break word\n",
    "        if answer_end >= answer_start:\n",
    "            answer = tokens[answer_start]\n",
    "            for i in range(answer_start + 1, answer_end + 1):\n",
    "                if tokens[i][0:2] == \"##\":\n",
    "                    answer = \"\"\n",
    "                else:\n",
    "                    answer += \" \" + tokens[i]\n",
    "        '''     #In our method, we use other classifier to do this\n",
    "        if answer.startswith(\"[CLS]\"):\n",
    "            # CLS means Unable to find the answer to your question.\n",
    "            answer = \"\"\n",
    "        '''\n",
    "        answers_list.append(answer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_qa       #del model release vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question answering complete, now for classifier to decide what question is \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_devset = pd.read_csv(\"dev-v2.0-combined-use.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_model = BertForSequenceClassification.from_pretrained(\n",
    "    './bert_qa_classifier_pt_3/', # 12-layer BERT\n",
    "    num_labels = 2, #0:false 1:true\n",
    "    output_attentions = False, # no attention output\n",
    "    output_hidden_states = False, # no need for classifier\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.to(device)    #put classifier model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "for questions, context in zip(classifier_devset['question'], classifier_devset['context']):\n",
    "\n",
    "    tokenized = tokenizer.encode_plus(questions,context,\n",
    "                            add_special_tokens=True,    # Add `[CLS]` and `[SEP]`\n",
    "                            truncation=True,\n",
    "                            return_attention_mask=True,  # Construct attn. masks.\n",
    "                            padding='max_length',       #512\n",
    "                            max_length=512)\n",
    "    \n",
    "    input_ids_list.append(tokenized['input_ids'])\n",
    "    attention_mask_list.append(tokenized['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids_list)\n",
    "attention_masks = torch.tensor(attention_mask_list)\n",
    "\n",
    "\n",
    "data_2_elements = torch.utils.data.TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "classifier_batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(\n",
    "            data_2_elements,  # The training samples.\n",
    "            sampler = None,\n",
    "            batch_size = classifier_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to classify\n",
    "from tqdm.auto import tqdm\n",
    "is_answerable_logits_list = []\n",
    "loop = tqdm(dataloader)\n",
    "for batch in loop:\n",
    "    batch_ids = batch[0].to(device)\n",
    "    batch_mask = batch[1].to(device)\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "\n",
    "        output = classifier_model(input_ids=batch_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=batch_mask \n",
    "                                )\n",
    "\n",
    "    #get predict logits and move it with true label to cpu\n",
    "    logits = output['logits']\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    is_answerable_logits_list.append(logits)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_output = []\n",
    "id_counter = 0\n",
    "answers_list\n",
    "for batch_losgits in is_answerable_logits_list:\n",
    "    for logit in batch_losgits:\n",
    "        \n",
    "        result_0_or_1 = np.argmax(logit, axis=None).flatten()      #0 false 1 true\n",
    "        if result_0_or_1[0] == 0:\n",
    "            answers_list[id_counter] = ''      #0 means unanswerable question, so ''\n",
    "\n",
    "        temp_output = {question_id_list[id_counter]: answers_list[id_counter]}\n",
    "\n",
    "        all_output.append(temp_output)\n",
    "        id_counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write result to json file it should be in wrong format\n",
    "with open(output_name, \"w\") as outfile:\n",
    "    json.dump(all_output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json():\n",
    "    f = open(output_name, \"r\")\n",
    "    line = f.read()\n",
    "    f.close()\n",
    "\n",
    "    line = line.replace(\"[\", \"\")\n",
    "    line = line.replace(\"]\", \"\")\n",
    "    line = line.replace(\"{\", \"\")\n",
    "    line = line.replace(\"}\", \"\")\n",
    "\n",
    "    line = \"{\" + line + \"}\"\n",
    "    \n",
    "    f = open(output_name, \"w\")\n",
    "    f.write(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix bad formatted json\n",
    "fix_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
