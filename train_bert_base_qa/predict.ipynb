{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for showing progress bar\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#Using torch by GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Use cuda device:\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"use cpu\")\n",
    "\n",
    "#set dataset location\n",
    "SQuAD = pd.read_csv('dev-v2.0.csv')\n",
    "#setoutput file name\n",
    "output_name = \"output-dev.json\"\n",
    "#checkpoint path\n",
    "model = BertForQuestionAnswering.from_pretrained('./bert_qa_pt_3/', local_files_only=True)\n",
    "#tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "#set batch_size here\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(dataset):\n",
    "    #convet values from dataframe to list for tokenizer\n",
    "    questions = dataset['question'].values.tolist()\n",
    "    contexts = dataset['context'].values.tolist()\n",
    "    id = dataset['id'].values.tolist()\n",
    "    #answers = dataset['text']\n",
    "    \n",
    "    return {\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'id': id,\n",
    "        #'answers': answers\n",
    "    }\n",
    "\n",
    "dataset = prep_data(SQuAD)\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "question_id_list = []\n",
    "type_ids_list = []\n",
    "\n",
    "for dataset_question, dataset_context in zip(dataset['question'], dataset['context']):\n",
    "    #do two times as by one line cannot get all values\n",
    "    \n",
    "    tokenized__for_index = tokenizer.encode(dataset_context,dataset_question,truncation=True,padding='max_length')                                 \n",
    "    tokenized = tokenizer.encode_plus(dataset_context,\n",
    "                            dataset_question,\n",
    "                            add_special_tokens=True,    # Add `[CLS]` and `[SEP]`\n",
    "                            truncation=True,\n",
    "                            return_attention_mask=True,  # Construct attn. masks.\n",
    "                            padding='max_length',\n",
    "                            return_tensors='pt')\n",
    "    \n",
    "    #find location of sep\n",
    "    sep_index = tokenized__for_index.index(tokenizer.sep_token_id)\n",
    "\n",
    "    #segment a which contains sep itself\n",
    "    segment_a_no = sep_index + 1\n",
    "\n",
    "    #segment b the rest of the ids\n",
    "    segment_b_no = len(tokenized__for_index) - segment_a_no\n",
    "\n",
    "    #make a list, 0 for segment a, 1 for segment b.\n",
    "    segment_ids = [0] * segment_a_no + [1] * segment_b_no\n",
    "    #padding is handled by attention mask\n",
    "    #check if segment_ids is normal\n",
    "    assert len(tokenized__for_index) == len(segment_ids)\n",
    "\n",
    "    type_ids = segment_ids\n",
    "    type_ids_list.append(type_ids)\n",
    "    input_ids_list.append(tokenized['input_ids'])\n",
    "    attention_mask_list.append(tokenized['attention_mask'])\n",
    "\n",
    "for id in dataset['id']:\n",
    "    question_id_list.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(map(len,type_ids_list)))==1:\n",
    "    print(\"All are the same length\")\n",
    "else:\n",
    "    print(\"They are not the same length!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question id list making\n",
    "for id in dataset['id']:\n",
    "    question_id_list.append(id)\n",
    "    \n",
    "#to tensor\n",
    "input_ids_list = torch.stack(input_ids_list)\n",
    "attention_mask_list = torch.stack(attention_mask_list)\n",
    "type_ids_list = torch.Tensor(type_ids_list).to(torch.int64)\n",
    "\n",
    "#pack dataset into data loader\n",
    "data_3_elements = TensorDataset(input_ids_list, attention_mask_list, type_ids_list)\n",
    "dataloader = DataLoader(data_3_elements, sampler=None, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)        #send model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run and get result from model \n",
    "all_output = []\n",
    "id_counter = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    #batch = torch.stack(batch)\n",
    "    #batch = batch.to(device)\n",
    "    \n",
    "    batch_input, batch_mask, batch_type = tuple(v.to(device) for v in batch)\n",
    "    batch_input.to(device)\n",
    "    batch_mask.to(batch_mask)\n",
    "    batch_mask.to(batch_type)\n",
    "    for input, mask, type in zip(batch_input, batch_mask, batch_type):\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input[0])  #[1] is device=\"device\"\n",
    "        result = model(input_ids = input, attention_mask=mask, token_type_ids=type)       #use ids and mask and type to get result\n",
    "        answer_start = torch.argmax(result.start_logits)    #get start position by argmax\n",
    "        answer_end = torch.argmax(result.end_logits)        #get end position by argmax\n",
    "\n",
    "        # join the words\n",
    "        answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "        if answer.startswith(\"[CLS]\"):\n",
    "            # CLS means Unable to find the answer to your question.\n",
    "            answer = \"\"\n",
    "\n",
    "\n",
    "        temp_output = {question_id_list[id_counter]: answer}\n",
    "\n",
    "        all_output.append(temp_output)\n",
    "        id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write result to json file it should be in wrong format\n",
    "with open(output_name, \"w\") as outfile:\n",
    "    json.dump(all_output, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json():\n",
    "    f = open(output_name, \"r\")\n",
    "    line = f.read()\n",
    "    f.close()\n",
    "\n",
    "    line = line.replace(\"[\", \"\")\n",
    "    line = line.replace(\"]\", \"\")\n",
    "    line = line.replace(\"{\", \"\")\n",
    "    line = line.replace(\"}\", \"\")\n",
    "\n",
    "    line = \"{\" + line + \"}\"\n",
    "    \n",
    "    f = open(output_name, \"w\")\n",
    "    f.write(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix bad formatted json\n",
    "fix_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
